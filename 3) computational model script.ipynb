{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "756c72f6-631b-4989-9336-d547ef0d5ce0",
   "metadata": {},
   "source": [
    "# Supplemental Code\n",
    "- Inherent instability of simple DNA repeats shapes an evolutionarily stable distribution of repeat lengths \n",
    "- McGinty et al. 2025\n",
    "- Part 3 of 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d37459-9d47-4f21-8c70-601428918d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "# note: pandas interpolate function has a hidden dependency for scipy\n",
    "\n",
    "import multiprocessing    # note: if using Windows, multiprocessing requires running Jupyter notebook via WSL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd351dcb-032e-425f-8f5f-80fde5630285",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e251923-82c2-472e-9e2d-9d4d8163624d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data generated in Notebooks 1 and 2\n",
    "# Can be skipped if running demo\n",
    "CHM13_counts = pd.read_pickle('repeat_distributions/CHM13_counts.pickle')\n",
    "CHM13_counts_B = pd.read_pickle('repeat_distributions/CHM13_counts_Bdist.pickle')\n",
    "random_counts = pd.read_pickle('repeat_distributions/random_counts.pickle')\n",
    "random_counts_B = pd.read_pickle('repeat_distributions/random_counts_Bdist.pickle')\n",
    "\n",
    "denovo_exp_rate = pd.read_pickle('denovo/denovo_exp_rate_remake.pickle')\n",
    "denovo_con_rate = pd.read_pickle('denovo/denovo_con_rate_remake.pickle')\n",
    "denovo_nonexp_rate = pd.read_pickle('denovo/denovo_nonexp_rate_remake.pickle')\n",
    "\n",
    "denovo_exp_rate_poisson = pd.read_pickle('denovo/denovo_exp_rate_poisson_remake.pickle')\n",
    "denovo_con_rate_poisson = pd.read_pickle('denovo/denovo_con_rate_poisson_remake.pickle')\n",
    "denovo_nonexp_rate_poisson = pd.read_pickle('denovo/denovo_nonexp_rate_poisson_remake.pickle')\n",
    "\n",
    "denovo_substitution_context_rate = pd.read_pickle('denovo/denovo_mut_freq_all.pickle')\n",
    "denovo_substitution_context_rate_poisson = pd.read_pickle('denovo/denovo_mut_freq_all_poisson.pickle')\n",
    "\n",
    "rates_mu_nu = pd.read_pickle('denovo/denovo_mut_freq_mu_nu_remake.pickle')\n",
    "\n",
    "intercept_list = dict()\n",
    "intercept_list['A'] = [denovo_con_rate['A'][8]] + [denovo_exp_rate['A'][8] * (denovo_exp_rate['A'][8]/ denovo_con_rate['A'][8])**x for x in range(7)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e315e2-bfae-4f6d-92a4-98e7b362ad88",
   "metadata": {},
   "source": [
    "##### skip this section if file has not yet been generated (see below in Notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c39ec6b-c2e3-4b27-abca-11f6f1884e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "subonly_counts = pd.read_pickle('repeat_distributions/subonly_counts_remake.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca88c40c-140f-4f56-a35b-d7ef1867decd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniform distribution for L=6-70\n",
    "counts_uniformrange = subonly_counts.copy().T\n",
    "for i in range(6,71):\n",
    "    counts_uniformrange[i] = counts_uniformrange[10].copy()\n",
    "counts_uniformrange = counts_uniformrange.T\n",
    "counts_uniformrange['A'] = counts_uniformrange['A'] * (subonly_counts.sum()['A'] / counts_uniformrange.sum()['A'])\n",
    "counts_uniformrange['B'] = subonly_counts['B'].copy()\n",
    "counts_uniformrange = counts_uniformrange.copy()\n",
    "counts_uniformrange.to_pickle('repeat_distributions/subonly_counts_uniformrange.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432dc674-dedb-4aa3-b562-a6dd7b3bdcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_uniform = pd.read_pickle('repeat_distributions/subonly_counts_uniformrange.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acc4505-eea9-4cf7-9ee8-fb8b920b0a43",
   "metadata": {},
   "source": [
    "#### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c76f99b-3c02-4754-809d-330c02b7aa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evolve function\n",
    "def mut_evolve_dist_AB(A_count_input, B_count_input, starting_conditions, boot = None, input_nuc = 'A', mut = True, mutonly = False, speedup_multiplier = 1, output_components = False, stochastics = None, reflective = True, boundary_count = 1000, first_bin = 0):\n",
    "    exp_rate_A_AA, con_rate_A_AA, nonexp_rate_A_AB, B_indel_rates = starting_conditions\n",
    "    A_count_output = A_count_input.copy(); B_count_output = B_count_input.copy()\n",
    "    A_bins = len(A_count_input); B_bins = len(B_count_input)\n",
    "    boundary_flag = False\n",
    "    A_length_array = np.array(range(1,A_bins+3))\n",
    "    A_length_array_bases = np.array(range(1,A_bins+3)) * len(input_nuc) ### including motif length\n",
    "    B_length_array = np.array(range(1,B_bins+3))\n",
    "    B_length_array_bases = np.array(range(1,B_bins+3)) * len(input_nuc) ### including motif length\n",
    "    A_count_input = np.insert(A_count_input, A_bins, [0,0])\n",
    "    B_count_input = np.insert(B_count_input, B_bins, [0,0])\n",
    "\n",
    "    if boot is None:\n",
    "        denovo_sub = denovo_substitution_context_rate.loc[input_nuc]\n",
    "    else:\n",
    "        denovo_sub = denovo_substitution_context_rate_poisson.loc[boot].loc[input_nuc]\n",
    "\n",
    "    # distribution info\n",
    "    total_B_bases = (B_count_input[:B_bins] * B_length_array_bases[:B_bins]).sum()\n",
    "    B_L1_base_portion = ((B_count_input[0] * len(input_nuc)) / (B_count_input[:B_bins]* B_length_array_bases[:B_bins]).sum()) ### including motif length\n",
    "    B_nonflank_base_portion = (B_count_input[2:B_bins+2] * B_length_array_bases[:B_bins]).sum() / total_B_bases  ### include portion of triplets 1nt away???\n",
    "    B_flank_base_portion = (B_count_input[1:B_bins] * 2 * len(input_nuc)).sum() / total_B_bases ### including motif length\n",
    "    \n",
    "    total_A_bases = (A_count_input[:A_bins] * A_length_array_bases[:A_bins]).sum()\n",
    "    A_nonflank_base_portion = (A_count_input[2:A_bins+2] * A_length_array_bases[:A_bins]).sum() / total_A_bases\n",
    "    A_flank_base_portion = (A_count_input[1:A_bins] * 2 * len(input_nuc)).sum() / total_A_bases ### including motif length\n",
    "    \n",
    "    total_A_change_in = np.array([0.0]*A_bins); total_B_change_in = np.array([0.0]*B_bins)\n",
    "    total_A_change_out = np.array([0.0]*A_bins); total_B_change_out = np.array([0.0]*B_bins)\n",
    "\n",
    "    if mut == True:\n",
    "        # A>B which adds to the A count locally. add these to A\n",
    "        A_mut_in_local_A_B = 2 * len(input_nuc) * denovo_sub['Acontraction'] * A_count_input[1:]\n",
    "        A_mut_out_local_A_B = -2 * len(input_nuc) * denovo_sub['Acontraction'] * A_count_input\n",
    "        A_mut_out_local_A_B[0] = -1 * len(input_nuc) * A_count_input[0] * denovo_sub['A10']\n",
    "\n",
    "        # total number of A>B fission events\n",
    "        A_mut_out_fission = np.insert((-denovo_sub['Afission'] * A_count_input[2:] * A_length_array_bases[:A_bins]), 0, [0, 0]) # used to subtract from A_count, starting from L=3 (with 0 for L=1,2)\n",
    "        # each fission creates 2 As. add these to A\n",
    "        A_mut_in_fission = ((2/A_length_array[:A_bins]) * -A_mut_out_fission[2:A_bins+2])[::-1].cumsum()[::-1]\n",
    " \n",
    "        # B>A which adds to the A count locally (which must come from B_L>1)\n",
    "        # A from B>A leaving the -1 bin\n",
    "        A_len_freq = (A_count_input / A_count_input.sum())[:A_bins]\n",
    "        A_mut_out_local_B_A = -denovo_sub['Aexpansion'] * B_flank_base_portion * total_B_bases * A_len_freq\n",
    "        # B>A creating A_L=1 from B_L>2\n",
    "        B_A_into_L1 = total_B_bases * B_nonflank_base_portion * denovo_sub['A01']\n",
    "        A_mut_in_local_B_A = np.insert(-A_mut_out_local_B_A, 0, B_A_into_L1)\n",
    "        \n",
    "        # fusion process for A\n",
    "        A_len_freq = (A_count_input / A_count_input.sum())[:A_bins]\n",
    "        A_fusion_freq_in = np.bincount((np.add.outer(A_length_array[:A_bins], A_length_array[:A_bins])+1).ravel(), weights = np.outer(A_len_freq, A_len_freq).ravel())[1:]\n",
    "        A_mut_in_fusion_A_B = A_fusion_freq_in * denovo_sub['Afusion'] * B_L1_base_portion * total_B_bases\n",
    "        A_mut_out_fusion_A_B = (-2) *A_len_freq * denovo_sub['Afusion'] * B_L1_base_portion * total_B_bases\n",
    "        \n",
    "        # total B>A\n",
    "        # B>A which adds to the B count locally. add these to B\n",
    "        B_mut_in_local_B_A = 2 * len(input_nuc) * denovo_sub['Aexpansion'] * B_count_input[1:]\n",
    "        B_mut_out_local_B_A = -2 * len(input_nuc) * denovo_sub['Aexpansion'] * B_count_input\n",
    "        B_mut_out_local_B_A[0] = -1 * B_L1_base_portion * total_B_bases * denovo_sub['Afusion']\n",
    "\n",
    "        # total number of B>A fission events\n",
    "        B_mut_out_fission = np.insert((-denovo_sub['A01'] * B_count_input[2:] * B_length_array_bases[:B_bins]), 0, [0, 0]) # used to subtract from B_count, starting from L=3 (with 0 for L=1,2)\n",
    "        # each fission creates 2 Bs. add these to B\n",
    "        B_mut_in_fission = ((2/B_length_array[:B_bins]) * -B_mut_out_fission[2:B_bins+2])[::-1].cumsum()[::-1]\n",
    "\n",
    "        # A>B which adds to the B count locally (which must come from A_L>1)\n",
    "        # B from A>B leaving the -1 bin\n",
    "        B_len_freq = (B_count_input / B_count_input.sum())[:B_bins]\n",
    "        B_mut_out_local_A_B = -denovo_sub['Acontraction'] * A_flank_base_portion * total_A_bases * B_len_freq\n",
    "        # A>B creating B_L=1 from A_L>2\n",
    "        A_B_into_L1 = total_A_bases * A_nonflank_base_portion * denovo_sub['Afission']\n",
    "        B_mut_in_local_A_B = np.insert(-B_mut_out_local_A_B, 0, A_B_into_L1)\n",
    "        \n",
    "        # fusion process for B\n",
    "        B_len_freq = (B_count_input / B_count_input.sum())[:B_bins]\n",
    "        B_fusion_freq_in = np.bincount((np.add.outer(B_length_array[:B_bins], B_length_array[:B_bins])+1).ravel(), weights = np.outer(B_len_freq, B_len_freq).ravel())[1:]\n",
    "        B_mut_in_fusion_B_A = B_fusion_freq_in * denovo_sub['A10'] * A_count_input[0] * len(input_nuc)\n",
    "        B_mut_out_fusion_B_A = (-2) * B_len_freq * denovo_sub['A10'] * A_count_input[0] * len(input_nuc)\n",
    "\n",
    "        # update counts for next round (with absorbing boundary)\n",
    "        total_A_change_in += A_mut_in_local_A_B[:A_bins] + A_mut_in_local_B_A[:A_bins] + A_mut_in_fission[:A_bins] + A_mut_in_fusion_A_B[:A_bins]\n",
    "        total_B_change_in += B_mut_in_local_B_A[:B_bins] + B_mut_in_local_A_B[:B_bins] + B_mut_in_fission[:B_bins] + B_mut_in_fusion_B_A[:B_bins]\n",
    "        total_A_change_out += A_mut_out_local_A_B[:A_bins] + A_mut_out_local_B_A[:A_bins] + A_mut_out_fission[:A_bins] + A_mut_out_fusion_A_B[:A_bins]\n",
    "        total_B_change_out += B_mut_out_local_B_A[:B_bins] + B_mut_out_local_A_B[:B_bins] + B_mut_out_fission[:B_bins] + B_mut_out_fusion_B_A[:B_bins]\n",
    "\n",
    "        # apply reflecting boundary\n",
    "        if reflective == True:\n",
    "            total_A_change_in[A_bins-1] += A_mut_in_local_A_B[A_bins:].sum() + A_mut_in_local_B_A[A_bins:].sum() + A_mut_in_fission[A_bins:].sum() + A_mut_in_fusion_A_B[A_bins:].sum()\n",
    "            total_B_change_in[B_bins-1] += B_mut_in_local_B_A[B_bins:].sum() + B_mut_in_local_A_B[B_bins:].sum() + B_mut_in_fission[B_bins:].sum() + B_mut_in_fusion_B_A[B_bins:].sum()\n",
    "            total_A_change_out[A_bins-1] += A_mut_out_local_A_B[A_bins:].sum() + A_mut_out_local_B_A[A_bins:].sum() + A_mut_out_fission[A_bins:].sum() + A_mut_out_fusion_A_B[A_bins:].sum()\n",
    "            total_B_change_out[B_bins-1] += B_mut_out_local_B_A[B_bins:].sum() + B_mut_out_local_A_B[B_bins:].sum() + B_mut_out_fission[B_bins:].sum() + B_mut_out_fusion_B_A[B_bins:].sum()\n",
    "           \n",
    "    if mutonly == False:\n",
    "        # A expansions in and out\n",
    "        A_exp_out = A_count_input[:A_bins] * -exp_rate_A_AA[:A_bins]\n",
    "        A_exp_in = np.insert(-A_exp_out, 0, B_indel_rates[0]*total_B_bases)\n",
    "\n",
    "        # A contractions in and out\n",
    "        A_con_out = A_count_input[:A_bins+1] * -con_rate_A_AA[:A_bins+2]\n",
    "        A_con_in = -A_con_out[1:]\n",
    "\n",
    "        # A fusions from B1>B0 deletions\n",
    "        if (mut != True):\n",
    "            A_len_freq = (A_count_input / A_count_input.sum())[:A_bins]\n",
    "            A_fusion_freq_in = np.bincount((np.add.outer(A_length_array[:A_bins], A_length_array[:A_bins])+1).ravel(), weights = np.outer(A_len_freq, A_len_freq).ravel())[1:]\n",
    "        A_mut_in_fusion_Bdel = A_fusion_freq_in[1:A_bins+1] * B_indel_rates[1] * B_L1_base_portion * total_B_bases\n",
    "        A_mut_out_fusion_Bdel = (-2) *A_len_freq * B_indel_rates[1] * B_L1_base_portion * total_B_bases\n",
    "\n",
    "        # A fission events from insertions\n",
    "        A_nonexp_out_fission = -A_count_input * nonexp_rate_A_AB # used to calculate fission_in, starting with L=2 going to 2x L=1\n",
    "        # each fission creates 2 As. add these to A\n",
    "        A_nonexp_in_fission = ((2/A_length_array[:A_bins]) * -A_nonexp_out_fission[1:A_bins+1])[::-1].cumsum()[::-1]\n",
    "\n",
    "        # B expansions in and out\n",
    "        B_exp_out = B_count_input[:B_bins] * -B_indel_rates[2] * B_length_array[:B_bins] # B>BB rates are flat, per base\n",
    "        B_exp_in = np.insert(-B_exp_out, 0, A_nonexp_out_fission.sum())\n",
    "\n",
    "        # B contractions in and out\n",
    "        B_con_out = B_count_input[:B_bins+1] * -B_indel_rates[1] * B_length_array[:B_bins+1] # B>_ rates are flat, per base\n",
    "        B_con_in = -B_con_out[1:]\n",
    "\n",
    "        # B fusions from A1>A0 deletions\n",
    "        if (mut != True):\n",
    "            B_len_freq = (B_count_input / B_count_input.sum())[:B_bins]\n",
    "            B_fusion_freq_in = np.bincount((np.add.outer(B_length_array[:B_bins], B_length_array[:B_bins])+1).ravel(), weights = np.outer(B_len_freq, B_len_freq).ravel())[1:]\n",
    "        B_mut_in_fusion_Adel = B_fusion_freq_in[1:B_bins+1] * A_count_input[0] * con_rate_A_AA[0]\n",
    "        B_mut_out_fusion_Adel = 2 *B_len_freq * A_count_input[0] * -con_rate_A_AA[0]\n",
    "                    \n",
    "        # B fission events from insertions\n",
    "        B_nonexp_out_fission = -B_count_input * B_indel_rates[0] * B_length_array # used to calculate fission_in, starting with L=2 going to 2x L=1\n",
    "        # each fission creates 2 Bs. add these to B\n",
    "        B_nonexp_in_fission = ((2/B_length_array[:B_bins]) * -B_nonexp_out_fission[1:B_bins+1])[::-1].cumsum()[::-1]\n",
    "\n",
    "       # update counts for next round (with absorbing boundary)\n",
    "        total_A_change_in += A_exp_in[:A_bins] + A_con_in[:A_bins] + A_mut_in_fusion_Bdel[:A_bins] + A_nonexp_in_fission[:A_bins]\n",
    "        total_B_change_in += B_exp_in[:B_bins] + B_con_in[:B_bins] + B_mut_in_fusion_Adel[:B_bins] + B_nonexp_in_fission[:B_bins]\n",
    "        total_A_change_out += A_exp_out[:A_bins] + A_con_out[:A_bins] + A_mut_out_fusion_Bdel[:A_bins] + A_nonexp_out_fission[:A_bins]\n",
    "        total_B_change_out += B_exp_out[:B_bins] + B_con_out[:B_bins] + B_mut_out_fusion_Adel[:B_bins] + B_nonexp_out_fission[:B_bins]\n",
    "\n",
    "        # apply reflecting boundary\n",
    "        if reflective == True:\n",
    "            total_A_change_in[A_bins-1] += A_exp_in[A_bins:].sum() + A_con_in[A_bins:].sum() + A_mut_in_fusion_Bdel[A_bins:].sum() + A_nonexp_in_fission[A_bins:].sum()\n",
    "            total_B_change_in[B_bins-1] += B_exp_in[B_bins:].sum() + B_con_in[B_bins:].sum() + B_mut_in_fusion_Adel[B_bins:].sum() + B_nonexp_in_fission[B_bins:].sum()\n",
    "            total_A_change_out[A_bins-1] += A_exp_out[A_bins:].sum() + A_con_out[A_bins:].sum() + A_mut_out_fusion_Bdel[A_bins:].sum() + A_nonexp_out_fission[A_bins:].sum()\n",
    "            total_B_change_out[B_bins-1] += B_exp_out[B_bins:].sum() + B_con_out[B_bins:].sum() + B_mut_out_fusion_Adel[B_bins:].sum() + B_nonexp_out_fission[B_bins:].sum()\n",
    "\n",
    "    # flag to stop the simulation if more repeats are removed from a bin than exist in that bin (excluding the last 10 noisy bins)\n",
    "    flag = ((np.abs(total_A_change_out[:A_bins-10]) * speedup_multiplier > A_count_output[:A_bins-10]).sum()) > 0\n",
    "\n",
    "    # apply speedup\n",
    "    total_A_change_in *= speedup_multiplier; total_A_change_out *= speedup_multiplier\n",
    "    total_B_change_in *= speedup_multiplier; total_B_change_out *= speedup_multiplier\n",
    "    \n",
    "    if stochastics is not None:\n",
    "        # the sum of poisson random variables is poisson-distributed. not necessary to run n poisson samples\n",
    "        total_A_change_in = np.random.poisson(total_A_change_in.clip(0))\n",
    "        total_A_change_out = -1 * np.random.poisson(np.abs(total_A_change_out.clip(max=0)))\n",
    "        total_B_change_in = np.random.poisson(total_B_change_in.clip(0))\n",
    "        total_B_change_out = -1 * np.random.poisson(np.abs(total_B_change_out.clip(max=0)))   \n",
    "    \n",
    "    total_A_change = total_A_change_in + total_A_change_out\n",
    "    total_B_change = total_B_change_in + total_B_change_out\n",
    "\n",
    "    if first_bin != 0:\n",
    "        total_A_change[:first_bin] = 0; total_B_change[:first_bin] = 0\n",
    "        \n",
    "    # update counts for next round\n",
    "    A_count_output = A_count_output[:A_bins] + total_A_change[:A_bins]\n",
    "    B_count_output = B_count_output[:B_bins] + total_B_change[:B_bins]\n",
    "\n",
    "    # remove negative values\n",
    "    A_count_output[A_count_output <0] = 0            \n",
    "    B_count_output[B_count_output <0] = 0\n",
    "\n",
    "    boundary_flag = ((A_count_output[A_bins-1] > boundary_count) | (B_count_output[B_bins-1] > boundary_count))\n",
    "    \n",
    "    if output_components == True:\n",
    "        if mutonly == False:\n",
    "            return  A_mut_in_local_A_B[:A_bins], A_mut_out_local_A_B[:A_bins], A_mut_in_local_B_A[:A_bins], A_mut_out_local_B_A[:A_bins], A_mut_in_fission[:A_bins], A_mut_out_fission[:A_bins], A_mut_in_fusion_A_B[:A_bins], A_mut_out_fusion_A_B[:A_bins], A_exp_in[:A_bins], A_exp_out[:A_bins], A_con_in[:A_bins], A_con_out[:A_bins], A_mut_in_fusion_Bdel[:A_bins], A_mut_out_fusion_Bdel[:A_bins], A_nonexp_in_fission[:A_bins], A_nonexp_out_fission[:A_bins]\n",
    "        else:\n",
    "            return  A_mut_in_local_A_B[:A_bins], A_mut_out_local_A_B[:A_bins], A_mut_in_local_B_A[:A_bins], A_mut_out_local_B_A[:A_bins], A_mut_in_fission[:A_bins], A_mut_out_fission[:A_bins], A_mut_in_fusion_A_B[:A_bins], A_mut_out_fusion_A_B[:A_bins]\n",
    "    else:\n",
    "        return A_count_output, B_count_output, flag, boundary_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e938c7-bcaf-4ebb-ae4e-d2392a854c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pin_power_law(power, pin_rate, pin_len=9, start_len=1, end_len=200):\n",
    "    denom = (pin_len**power) / pin_rate\n",
    "    return pd.Series([i**power for i in range(start_len, end_len+1)], index = list(range(start_len,end_len+1))) / denom\n",
    "\n",
    "def intercept_then_powerlaw(exp_power, con_power, exp_int, con_int, pin_len = 9, A_bins = 200, boot = None, motif = 'A', interp = False, nonexp_factor = 0.01, intercept_group = 'motif'):\n",
    "    if intercept_group == 'motif':\n",
    "        intercept_group = intercept_list[motif]\n",
    "    if boot is None:\n",
    "        bootname = ''\n",
    "        denovo_exp_rate_current = denovo_exp_rate[motif].copy()\n",
    "        denovo_con_rate_current = denovo_con_rate[motif].copy()\n",
    "        denovo_nonexp_rate_current = denovo_nonexp_rate[motif].copy()\n",
    "    else:\n",
    "        bootname = '_boot'+str(boot)\n",
    "        denovo_exp_rate_current = denovo_exp_rate_poisson[motif][boot].copy()\n",
    "        denovo_con_rate_current = denovo_con_rate_poisson[motif][boot].copy()\n",
    "        denovo_nonexp_rate_current = denovo_nonexp_rate_poisson[motif][boot].copy()\n",
    "    exp = pd.concat([denovo_exp_rate_current.reindex(range(pin_len)), pin_power_law(exp_power, intercept_group[exp_int], start_len=pin_len, end_len=A_bins+3)])\n",
    "    con = pd.concat([denovo_con_rate_current.reindex(range(pin_len)), pin_power_law(con_power, intercept_group[con_int], start_len=pin_len, end_len=A_bins+3)])\n",
    "    nonexp = pd.concat([denovo_nonexp_rate_current.reindex(range(pin_len)), pin_power_law(exp_power, intercept_group[exp_int] * nonexp_factor, start_len=pin_len, end_len=A_bins+3)])\n",
    "    nonexpname = '_nex' + str(nonexp_factor)\n",
    "    nonexp.loc[1] = 0\n",
    "    if interp == True:\n",
    "        interpname = '_interp'\n",
    "        exp.loc[8:13] = np.nan\n",
    "        con.loc[8:13] = np.nan\n",
    "        nonexp.loc[8:13] = np.nan\n",
    "        exp = exp.interpolate(method = 'quadratic')\n",
    "        con = con.interpolate(method = 'quadratic')\n",
    "        nonexp = nonexp.interpolate(method = 'quadratic')\n",
    "    else:\n",
    "        interpname = ''\n",
    "    name = '_interceptPL_tE' + str(exp_power) + '_tC' + str(con_power) +'_iE' + str(exp_int) + '_iC' + str(con_int) + nonexpname + interpname + bootname\n",
    "    return name, exp, con, nonexp\n",
    "\n",
    "#### alternate rate functional forms\n",
    "def power_law_rate_at_L(power, L, rate = 1e-8, bins = 200):\n",
    "    return pd.Series([np.exp(np.log(rate) - power * np.log(L) + np.log(length)*power) for length in range(1,bins)], index = range(1,bins))\n",
    "\n",
    "def power_law_rate_at_L_v2(power, Lam, mu = 4.117e-09, bins = 200):\n",
    "    return pd.Series([((2*mu)/Lam) * (length/Lam)**power for length in range(1,bins)], index = range(1,bins))\n",
    "\n",
    "def log_curve_from_L9(power, intercept, motif = 'A', bins = 200):\n",
    "    return pd.Series([intercept_list[motif][intercept] * (np.log(1+L-8) / np.log(1+9 -8))**power for L in range(9,bins+1)], index = range(9,bins+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e439c1-f9b1-47b3-877b-986bc93d207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup function\n",
    "def setup_evolve(exp_power=0, con_power=0, exp_int=0, con_int=0, boot = None, stochastics = None, interp = False, pin_len = 9, nonexp_factor = 0.01, A_bins = 200, B_bins = 200, input_nuc = 'A', mutonly = False, exp_zero = False, con_zero = False, nonexp_zero = False, starting_counts = 'random', ceiling = None, rates_function = 'powerlaw'):\n",
    "# set up counts\n",
    "    A_length_array = np.array(range(1,A_bins+1))\n",
    "    B_length_array = np.array(range(1,A_bins+1))\n",
    "    if starting_counts == 'random':\n",
    "        A_count_input = np.nan_to_num(random_counts[len(input_nuc)][input_nuc].reindex(range(1,A_bins+1)).values)\n",
    "        B_count_input = np.nan_to_num(random_counts_B[input_nuc].reindex(range(1,B_bins+1)).values)\n",
    "    if starting_counts == 'subonly':\n",
    "        A_count_input = np.nan_to_num(subonly_counts['A'][input_nuc].reindex(range(1,A_bins+1)).values)\n",
    "        B_count_input = np.nan_to_num(subonly_counts['B'][input_nuc].reindex(range(1,B_bins+1)).values)\n",
    "    if starting_counts == 'CHM13':\n",
    "        A_count_input = np.nan_to_num(CHM13_counts[len(input_nuc)][input_nuc].reindex(range(1,A_bins+1)).values)\n",
    "        B_count_input = np.nan_to_num(CHM13_counts_B[input_nuc].reindex(range(1,B_bins+1)).values)\n",
    "    if starting_counts == 'uniform':\n",
    "        A_count_input = np.nan_to_num(counts_uniform['A'][input_nuc].reindex(range(1,A_bins+1)).values)\n",
    "        B_count_input = np.nan_to_num(counts_uniform['B'][input_nuc].reindex(range(1,B_bins+1)).values)\n",
    "    if starting_counts not in ['random', 'subonly', 'CHM13', 'uniform']:\n",
    "        A_count_input = np.nan_to_num(starting_counts[0].reindex(range(1,A_bins+1)).values)\n",
    "        B_count_input = np.nan_to_num(starting_counts[1].reindex(range(1,B_bins+1)).values)\n",
    "# set up rates    \n",
    "    if mutonly == False:\n",
    "        if rates_function == 'powerlaw':\n",
    "            name, exp_rate, con_rate, nonexp_rate = intercept_then_powerlaw(exp_power = exp_power, con_power = con_power, exp_int = exp_int, con_int=con_int, pin_len=pin_len, A_bins = A_bins, boot = boot, motif = input_nuc, interp = interp, nonexp_factor = nonexp_factor)\n",
    "            B_indel_rate = np.array([exp_rate[0], con_rate[0], nonexp_rate[0]])        \n",
    "        if rates_function == 'powerlaw_x':\n",
    "            name = '_PL1e-8atL_tE' + str(exp_power) + '_tC' + str(con_power) +'_LE' + str(exp_int) + '_LC' + str(con_int)\n",
    "            exp_rate = power_law_rate_at_L(exp_power, exp_int, rate = rates_mu_nu['B>A'], bins = A_bins+3).reindex(range(A_bins+3))\n",
    "            con_rate = power_law_rate_at_L(con_power, con_int, rate = rates_mu_nu['A>B'], bins = A_bins+3).reindex(range(A_bins+3))\n",
    "            nonexp_rate = exp_rate * nonexp_factor\n",
    "            B_indel_rate = np.array([denovo_exp_rate[input_nuc][0], denovo_con_rate[input_nuc][0], denovo_nonexp_rate[input_nuc][0]])\n",
    "        if rates_function == 'powerlaw_lambda':\n",
    "            name = '_PLmuatL_tE' + str(exp_power) + '_tC' + str(con_power) +'_LE' + str(exp_int) + '_LC' + str(con_int)\n",
    "            exp_rate = power_law_rate_at_L_v2(exp_power, exp_int, mu = rates_mu_nu['B>A'], bins = A_bins+3).reindex(range(A_bins+3))\n",
    "            con_rate = power_law_rate_at_L_v2(con_power, con_int, mu = rates_mu_nu['B>A'], bins = A_bins+3).reindex(range(A_bins+3))\n",
    "            nonexp_rate = exp_rate * nonexp_factor\n",
    "            B_indel_rate = np.array([denovo_exp_rate[input_nuc][0], denovo_con_rate[input_nuc][0], denovo_nonexp_rate[input_nuc][0]])\n",
    "        if rates_function == 'log':\n",
    "            name = '_logrates_tE' + str(exp_power) + '_tC' + str(con_power) +'_iE' + str(exp_int) + '_iC' + str(con_int)\n",
    "            exp_rate = pd.concat([denovo_exp_rate['A'].loc[:8], log_curve_from_L9(exp_power, exp_int, A_bins+3)])\n",
    "            con_rate = pd.concat([denovo_con_rate['A'].loc[:8], log_curve_from_L9(con_power, con_int, A_bins+3)])\n",
    "            nonexp_rate = pd.concat([denovo_nonexp_rate['A'].loc[:8], log_curve_from_L9(exp_power, exp_int, A_bins+3) * nonexp_factor])\n",
    "            B_indel_rate = np.array([denovo_exp_rate[input_nuc][0], denovo_con_rate[input_nuc][0], denovo_nonexp_rate[input_nuc][0]])\n",
    "        if rates_function == 'custom':\n",
    "            exp_rate, con_rate, name = custom_rates\n",
    "            exp_rate = exp_rate.reindex(range(A_bins+3))\n",
    "            con_rate = con_rate.reindex(range(A_bins+3))\n",
    "            nonexp_rate = exp_rate * nonexp_factor\n",
    "            B_indel_rate = np.array([denovo_exp_rate[input_nuc][0], denovo_con_rate[input_nuc][0], denovo_nonexp_rate[input_nuc][0]])\n",
    "        # change rates from per unit to per STR\n",
    "        exp_rate = exp_rate.values[1:A_bins+1] * A_length_array\n",
    "        con_rate = con_rate.values[1:A_bins+2] * np.array(range(1,A_bins+2))\n",
    "        nonexp_rate = nonexp_rate.values[1:A_bins+3] * np.array(range(1,A_bins+3))\n",
    "        if ceiling != None:\n",
    "            ceiling_loc = []\n",
    "            if (exp_rate > ceiling).sum() > 0:\n",
    "                ceiling_loc.append(pd.Series(exp_rate > ceiling).idxmax())\n",
    "            if (con_rate > ceiling).sum() > 0:\n",
    "                ceiling_loc.append(pd.Series(con_rate > ceiling).idxmax())\n",
    "            if len(ceiling_loc) > 0:\n",
    "                ceiling_loc = min(np.array(ceiling_loc))\n",
    "                print( '\\r' + 'rate ceiling reached at L=' + str(ceiling_loc), end = ' ')\n",
    "                exp_rate[ceiling_loc:] = ceiling\n",
    "                con_rate[ceiling_loc:] = ceiling\n",
    "                nonexp_rate[ceiling_loc:] = ceiling\n",
    "            name = name + '_ceiling_' + str(ceiling)\n",
    "        if exp_zero == True:\n",
    "            exp_rate *= 0\n",
    "        if con_zero == True:\n",
    "            con_rate *= 0\n",
    "        if nonexp_zero == True:\n",
    "            nonexp_rate *= 0\n",
    "        if starting_counts == 'random':\n",
    "            name = name + '_randomstart'        \n",
    "        if starting_counts == 'subonly':\n",
    "            name = name + '_subonlystart'\n",
    "        if starting_counts == 'CHM13':\n",
    "            name = name + '_CHM13start'\n",
    "        if starting_counts == 'uniform':\n",
    "            name = name + '_uniformstart'\n",
    "        if stochastics is None:\n",
    "            name = name\n",
    "        else:\n",
    "            name = name + '_stochastics_' + str(stochastics)\n",
    "        return A_count_input, B_count_input, exp_rate, con_rate, nonexp_rate, B_indel_rate, name\n",
    "    else:\n",
    "        if boot is None:\n",
    "            name = 'mutonly'\n",
    "        else:\n",
    "            name = 'mutonly_boot' + str(boot)\n",
    "        if starting_counts == 'random':\n",
    "            name = name + '_randomstart'        \n",
    "        if starting_counts == 'subonly':\n",
    "            name = name + '_subonlystart'\n",
    "        if starting_counts == 'CHM13':\n",
    "            name = name + '_CHM13start'\n",
    "        if starting_counts == 'uniform':\n",
    "            name = name + '_uniformstart'\n",
    "        if stochastics is None:\n",
    "            name = name\n",
    "        else:\n",
    "            name = name + '_stochastics_' + str(stochastics)\n",
    "        return A_count_input, B_count_input, None, None, None, None, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bf245b-13f7-4e16-881a-e7df821a62dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation_constantspeedup(exp_int, con_int, exp_power, con_power, min_speedup = 1, rounds = 9, ceiling = 0.1, interp = False, nonexp_factor = 0.01, A_bins = 200, B_bins = 200, input_nuc = 'A', mutonly = False, stochastics = None, boot = None, boundary_count = 1000, overwrite = False, starting_counts = 'random', reflective = True, sim_dir = 'simulations/testing/', rates_function = 'powerlaw', first_bin = 0, write = True, startfrom = None, recnum = 11, pin_len = 9):\n",
    "    starting_conditions = setup_evolve(exp_power, con_power, exp_int, con_int, nonexp_factor = nonexp_factor, A_bins = A_bins, B_bins = B_bins, input_nuc = input_nuc, mutonly = mutonly, starting_counts = starting_counts, boot = boot, ceiling = None, interp = interp, rates_function = rates_function, pin_len = pin_len)\n",
    "\n",
    "    # set speedup to maximum allowable or reduce A_bins to avoid rate ceiling\n",
    "    if mutonly != True:\n",
    "        rates_sum = starting_conditions[2][:A_bins] + starting_conditions[3][:A_bins] + starting_conditions[4][:A_bins]\n",
    "    else:\n",
    "        rates_sum = denovo_substitution_context_rate.loc[input_nuc].sum() * 100\n",
    "    speedup = -2 - round(np.log10(rates_sum.max()) - 0.5)\n",
    "    if speedup < min_speedup:\n",
    "        speedup = min_speedup; A_bins = np.array(list(np.where(rates_sum * 10**speedup >= ceiling)[0])).min() -1\n",
    "        starting_conditions = setup_evolve(exp_power, con_power, exp_int, con_int, nonexp_factor = nonexp_factor, A_bins = A_bins, B_bins = B_bins, input_nuc = input_nuc, mutonly = mutonly, starting_counts = starting_counts, boot = boot, ceiling = None, interp = interp, rates_function = rates_function, pin_len = pin_len)\n",
    "    rounds = rounds - speedup # default is 10^9 generations = 10^(iterations+speedup)\n",
    "\n",
    "    if overwrite == False:\n",
    "        if 'Adist_'+input_nuc+'_bins'+str(A_bins)+'_sp1e'+str(speedup)+'_rounds1e'+str(rounds)+'_'+starting_conditions[6]+'.pickle' in os.listdir(sim_dir):\n",
    "            print('already done: ' + starting_conditions[6])\n",
    "            return None\n",
    "        else:\n",
    "            pass\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    if startfrom is None:\n",
    "        A_counts_timeseries = dict(); B_counts_timeseries = dict()\n",
    "        A_counts_timeseries[0] = starting_conditions[0][:A_bins]; B_counts_timeseries[0] = starting_conditions[1]\n",
    "        A_counts_current = A_counts_timeseries[0]; B_counts_current = B_counts_timeseries[0]; flag = False; boundary_flag = False\n",
    "        startrep = 0\n",
    "        print('\\r' + '         ' + starting_conditions[6], end = '     ')\n",
    "\n",
    "    else:\n",
    "        A_counts_timeseries, B_counts_timeseries = startfrom\n",
    "        startrep = list(A_counts_timeseries.keys())[-1]\n",
    "        A_counts_current = pd.Series(A_counts_timeseries[startrep]).reindex(range(A_bins)).fillna(0).values; B_counts_current = B_counts_timeseries[startrep]; flag = False; boundary_flag = False\n",
    "        \n",
    "    for rep in range(1, 1 + 10**rounds):\n",
    "        if (flag == False):\n",
    "            A_counts_current, B_counts_current, flag, boundary_flag = mut_evolve_dist_AB(A_counts_current, B_counts_current, starting_conditions[2:6], boot=boot, input_nuc = input_nuc, mutonly=mutonly, speedup_multiplier=10**speedup, stochastics = stochastics, reflective = reflective, boundary_count = boundary_count, first_bin = first_bin)\n",
    "            # save 10 evenly-spaced (in log scale) timepoints per log10 rounds\n",
    "            if rep in np.linspace(0,10**rounds,recnum).astype(int):\n",
    "                print('\\r' + str(rep), end = '   ')\n",
    "                A_counts_timeseries[startrep + rep * (10**speedup)], B_counts_timeseries[startrep + rep * (10**speedup)] = A_counts_current, B_counts_current\n",
    "        else:\n",
    "            print('\\r' + 'ending due to numerical error at round '+str(rep))\n",
    "            break\n",
    "    if write == True:\n",
    "        A_counts_timeseries = pd.DataFrame(A_counts_timeseries)\n",
    "        B_counts_timeseries = pd.DataFrame(B_counts_timeseries)\n",
    "        A_counts_timeseries.to_pickle(sim_dir + 'Adist_'+input_nuc+'_bins'+str(A_bins)+'_sp1e'+str(speedup)+'_rounds1e'+str(rounds)+'_'+starting_conditions[6]+'.pickle')\n",
    "        B_counts_timeseries.to_pickle(sim_dir + 'Bdist_'+input_nuc+'_bins'+str(A_bins)+'_sp1e'+str(speedup)+'_rounds1e'+str(rounds)+'_'+starting_conditions[6]+'.pickle')\n",
    "    return A_counts_timeseries, B_counts_timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4df84a-3c86-4a8e-a943-48805144abf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation_prospeedup(exp_int, con_int, exp_power, con_power, min_speedup = 3, interp = False, nonexp_factor = 0.01, A_bins = 200, B_bins = 200, input_nuc = 'A', mutonly = False, stochastics = None, boot = None, boundary_count = 1000, overwrite = False, starting_counts = 'subonly', reflective = True, sim_dir = 'simulations/testing/', rates_function = 'powerlaw', first_bin = 0, pin_len= 9):\n",
    "    starting_conditions = setup_evolve(exp_power, con_power, exp_int, con_int, nonexp_factor = nonexp_factor, A_bins = A_bins, B_bins = B_bins, input_nuc = input_nuc, mutonly = mutonly, starting_counts = starting_counts, ceiling = None, interp = interp, boot = boot, rates_function = rates_function, pin_len = pin_len)\n",
    "    if overwrite == False:\n",
    "        if 'Adist_'+input_nuc+'_prospeedup_'+starting_conditions[6]+'.pickle' in os.listdir(sim_dir):\n",
    "            print('already done: ' + starting_conditions[6])\n",
    "            return None\n",
    "        else:\n",
    "            pass\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # set speedup to minimum, then increase to maximum allowable or reduce A_bins to avoid rate ceiling\n",
    "    exp_rate = starting_conditions[2].copy(); con_rate = starting_conditions[3].copy()\n",
    "    max_speedup=0\n",
    "    exp_rate *= 10**max_speedup\n",
    "    con_rate *= 10**max_speedup\n",
    "    while exp_rate.max() + con_rate.max() < 0.01:\n",
    "        max_speedup += 1\n",
    "        exp_rate *= 10\n",
    "        con_rate *= 10\n",
    "    \n",
    "    A_counts_timeseries, B_counts_timeseries = run_simulation_constantspeedup(exp_int=exp_int, con_int=con_int, exp_power=exp_power, con_power=con_power, min_speedup = min_speedup, rounds = 9, ceiling = 0.1, interp = interp, nonexp_factor = nonexp_factor, A_bins = A_bins, B_bins = B_bins, input_nuc = input_nuc, mutonly = mutonly, stochastics = stochastics, boot = boot, boundary_count = boundary_count, overwrite = None, starting_counts = starting_counts, reflective = reflective, sim_dir = None, rates_function = rates_function, first_bin = first_bin, write = False, recnum = 5, pin_len = pin_len)\n",
    "    if max_speedup < min_speedup:\n",
    "        for speedup in list(range(max_speedup, min_speedup))[::-1]:\n",
    "            A_counts_timeseries, B_counts_timeseries = run_simulation_constantspeedup(exp_int=exp_int, con_int=con_int, exp_power=exp_power, con_power=con_power, min_speedup = speedup, rounds = 9 - min_speedup +speedup, ceiling = 0.1, interp = interp, nonexp_factor = nonexp_factor, A_bins = A_bins, B_bins = B_bins, input_nuc = input_nuc, mutonly = mutonly, stochastics = stochastics, boot = boot, boundary_count = boundary_count, overwrite = None, starting_counts = starting_counts, reflective = reflective, sim_dir = None, rates_function = rates_function, first_bin = first_bin, write = False, startfrom = (A_counts_timeseries, B_counts_timeseries), recnum = 5, pin_len = pin_len)\n",
    "            \n",
    "    A_counts_timeseries = pd.DataFrame.from_dict(A_counts_timeseries, orient = 'index').T\n",
    "    B_counts_timeseries = pd.DataFrame.from_dict(B_counts_timeseries, orient = 'index').T\n",
    "    A_counts_timeseries.to_pickle(sim_dir + 'Adist_'+input_nuc+'_prospeedup_'+starting_conditions[6]+'.pickle')\n",
    "    B_counts_timeseries.to_pickle(sim_dir + 'Bdist_'+input_nuc+'_prospeedup_'+starting_conditions[6]+'.pickle')\n",
    "    return A_counts_timeseries, B_counts_timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a09b3e5-1510-4c98-9c45-6d66d2d58102",
   "metadata": {},
   "source": [
    "#### input arguments and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8099be6b-c413-4c45-8ee8-d5643bae92a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for running script. ignore if running demo via Jupyter notebook\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='repeat distribution simulation')\n",
    "\n",
    "parser.add_argument('--dir', action=\"store\", dest='dir', default = 'simulations/grid_output/', type=str)\n",
    "parser.add_argument('--exp_p', action=\"store\", dest='exp_p', type=float)\n",
    "parser.add_argument('--con_p', action=\"store\", dest='con_p', type=float)\n",
    "parser.add_argument('--exp_i', action=\"store\", dest='exp_i', type=int)\n",
    "parser.add_argument('--con_i', action=\"store\", dest='con_i', type=int)\n",
    "parser.add_argument('--motif', action=\"store\", dest='motif', default = 'A', type=str)\n",
    "parser.add_argument('--speedup', action=\"store\", dest='speedup', default = 5, type=int)\n",
    "parser.add_argument('--rounds', action=\"store\", dest='rounds', default = 0, type=int)\n",
    "parser.add_argument('--A_bins', action=\"store\", dest='A_bins', default = 200, type=int)\n",
    "parser.add_argument('--B_bins', action=\"store\", dest='B_bins', default = 200, type=int)\n",
    "parser.add_argument('--boot', action=\"store\", dest='boot', type = int)\n",
    "parser.add_argument('--boundary_count', action=\"store\", dest='boundary_count', default = 1000, type=int)\n",
    "parser.add_argument('--mutonly', default=False, action=\"store_true\")\n",
    "parser.add_argument('--starting_counts', action=\"store\", dest='starting_counts', default = 'random', type=str)\n",
    "parser.add_argument('--overwrite', default=False, action=\"store_true\")\n",
    "parser.add_argument('--stochastics', action=\"store\", dest='stochastics', type = int)\n",
    "parser.add_argument('--reflective', default=True, action=\"store_false\")\n",
    "parser.add_argument('--ceiling', action=\"store\", dest='ceiling', default=None, type=float)\n",
    "parser.add_argument('--constantspeedup', default=False, action=\"store_true\")\n",
    "parser.add_argument('--interp', default=False, action=\"store_true\")\n",
    "parser.add_argument('--jobgroup', action=\"store\", dest='jobgroup', type=int)\n",
    "parser.add_argument('--rates_function', action=\"store\", dest='rates_function', default = 'powerlaw', type=str)\n",
    "parser.add_argument('--jobfile', action=\"store\", dest='jobfile', default = 'grid_group_jobs.pickle', type=str)\n",
    "parser.add_argument('--firstbin', action=\"store\", dest='firstbin', default = 0, type=int)\n",
    "parser.add_argument('--pin_len', action=\"store\", dest='pin_len', default = 9, type=int)\n",
    "parser.add_argument('--recnum', action=\"store\", dest='recnum', default = 11, type=int)\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "finished = os.listdir(args.dir)\n",
    "\n",
    "if args.constantspeedup == False:\n",
    "    if args.jobgroup is not None:\n",
    "        if args.jobgroup == -1:\n",
    "            grid_group = pd.read_pickle(args.jobfile)\n",
    "        else:\n",
    "            grid_group = pd.read_pickle(args.jobfile).loc[args.jobgroup]\n",
    "        if __name__ == '__main__':\n",
    "            pool = multiprocessing.Pool()\n",
    "            for exp_i, con_i, exp_p, con_p in grid_group:\n",
    "                pool.apply_async(run_simulation_prospeedup, args=(exp_i, con_i, exp_p, con_p, args.speedup, args.interp, 0.01, args.A_bins, args.B_bins, args.motif, args.mutonly, args.stochastics, args.boot, args.boundary_count, args.overwrite, args.starting_counts, args.reflective, args.dir, args.rates_function, args.firstbin, args.pin_len))\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "    else:\n",
    "        run_simulation_prospeedup(exp_power=args.exp_p, con_power=args.con_p, exp_int=args.exp_i, con_int=args.con_i, boot = args.boot, input_nuc = args.motif, A_bins = args.A_bins, B_bins = args.B_bins, mutonly = args.mutonly, starting_counts = args.starting_counts, min_speedup = args.speedup, sim_dir = args.dir, overwrite = args.overwrite, stochastics = args.stochastics, reflective = args.reflective, rates_function = args.rates_function, first_bin = args.firstbin)\n",
    "else:\n",
    "    if args.jobgroup is not None:\n",
    "        if args.jobgroup == -1:\n",
    "            grid_group = pd.read_pickle(args.jobfile)\n",
    "        else:\n",
    "            grid_group = pd.read_pickle(args.jobfile).loc[args.jobgroup]\n",
    "        if __name__ == '__main__':\n",
    "            pool = multiprocessing.Pool()\n",
    "            for exp_i, con_i, exp_p, con_p in grid_group:\n",
    "                pool.apply_async(run_simulation_constantspeedup, args=(exp_i, con_i, exp_p, con_p, args.speedup, args.rounds, args.ceiling, args.interp, 0.01, args.A_bins, args.B_bins, args.motif, args.mutonly, args.stochastics, args.boot, args.boundary_count, args.overwrite, args.starting_counts, args.reflective, args.dir, args.rates_function, args.firstbin, True, None, args.recnum, args.pin_len))\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "    else:\n",
    "        run_simulation_constantspeedup(exp_power=args.exp_p, con_power=args.con_p, exp_int=args.exp_i, con_int=args.con_i, boot = args.boot, input_nuc = args.motif, A_bins = args.A_bins, B_bins = args.B_bins, mutonly = args.mutonly, starting_counts = args.starting_counts, min_speedup = args.speedup, rounds = args.rounds, ceiling = args.ceiling, sim_dir = args.dir, overwrite = args.overwrite, stochastics = args.stochastics, reflective = args.reflective, rates_function = args.rates_function, first_bin = args.firstbin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5def57-cf22-421f-93b7-57d64d99d4c4",
   "metadata": {},
   "source": [
    "##### - remove all lines below to generate simulation_script_prospeedup.py\n",
    "- Generation of complete dataset requires using simulation_script_speedup.py along with jobs_prospeedup.sh (and simulation_script.py along with jobs_sp1_long.sh and jobs_sp2_long.sh) on a Slurm-based cluster to more quickly populate all points in the grid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8c3b38-be59-4620-b4fa-bb33256faea7",
   "metadata": {},
   "source": [
    "##### Arguments:\n",
    "- --dir -> directory where output of simulation will be stored\n",
    "- --exp_p -> power law exponent controlling expansion and non-motif insertion rates\n",
    "- --con_p -> power law exponent controlling contraction rate\n",
    "- --exp_i -> intercept of expansion rate power law (as the index of the 'intercept_list')\n",
    "- --con_i -> intercept of contraction rate power law (as the index of the 'intercept_list')\n",
    "- --motif -> STR sequence motif\n",
    "- --speedup -> controls constant or maximum speedup factor applied to all mutation rates\n",
    "- --rounds -> controlls number of iterations (10^n). with prospeedup==True, this adds rounds per speedup\n",
    "- --A_bins -> number of length bins 1-n for the A (repeat) motifs\n",
    "- --B_bins -> number of length bins 1-n for the B (non-repeat) strings\n",
    "- --boot -> selects from a pre-generated list of mutation rates subject to Poisson noise\n",
    "- --mutonly -> turns off all indel and instability processes in the simulation, running substituion processes only\n",
    "- --starting_counts -> choose the starting distribution. recognizes 'random', 'subonly', 'CHM13' and 'uniform', or supply (A_counts, B_counts)\n",
    "- --overwrite -> ignore check for whether the conditions will produce a file that already exists in the given directory\n",
    "- --stochastics -> turns on stochastic (Poisson) sampling of mutation counts during each iteration\n",
    "- -- ceiling -> set upper rate limit for any mutation process\n",
    "- --reflective -> turn off handling of the reflecting boundary at L=A_bins and B_bins, turning it into an absorbing boundary\n",
    "- --rates_function -> choose between several functions determining the expansion and contraction rates\n",
    "- --jobgroup -> run a list of jobs in parallel, according to a pre-made file specifying groups of parameter combinations\n",
    "- --jobfile -> location of pre-made file specifying groups of parameter combinations\n",
    "- --firstbin -> flag to ignore first n bins (testing purposes only)\n",
    "- --pin_len -> length bin at which to pin the power law model to the empirical rates (default is 9)\n",
    "- --recnum -> number of linearly-spaced timepoints to record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67cf3fe-5feb-4602-933d-9fd9c402932e",
   "metadata": {},
   "source": [
    "## Make job files for running grid on cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ce98ac-4745-45e4-ad17-33a8c7fe0770",
   "metadata": {},
   "source": [
    "### for power law 4 parameter model:\n",
    "- split into 1000 jobs by approximate time needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ca9d71-f62c-41ff-afee-7bacb819691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_constantspeedup(exp_int, con_int, exp_power, con_power, min_speedup = 0, rounds = 9, ceiling = 0.1, interp = False, nonexp_factor = 0.01, A_bins = 200, B_bins = 200, input_nuc = 'A', mutonly = False, stochastics = None, boot = None, boundary_count = 1000, overwrite = False, starting_counts = 'random', reflective = True, sim_dir = 'simulations/testing/', rates_function = 'powerlaw', first_bin = 0, write = True, startfrom = None, recnum = 10):\n",
    "    starting_conditions = setup_evolve(exp_power, con_power, exp_int, con_int, nonexp_factor = nonexp_factor, A_bins = A_bins, B_bins = B_bins, input_nuc = input_nuc, mutonly = mutonly, starting_counts = starting_counts, boot = boot, ceiling = None, interp = interp, rates_function = rates_function)\n",
    "\n",
    "    # set speedup to maximum allowable or reduce A_bins to avoid rate ceiling\n",
    "    rates_sum = starting_conditions[2][:A_bins] + starting_conditions[3][:A_bins] + starting_conditions[4][:A_bins]\n",
    "    speedup = -2 - round(np.log10(rates_sum.max()) - 0.5)\n",
    "    if speedup < min_speedup:\n",
    "        speedup = min_speedup; A_bins = np.array(list(np.where(rates_sum * 10**speedup >= ceiling)[0])).min() -1\n",
    "    return speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3ad3bc-db54-45da-990c-6e6eb6510420",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_fullgrid = pd.Series([(exp_c, con_c, exp_power, con_power) for exp_power in np.linspace(0,4,41).round(2) for exp_c in range(8) for con_power in np.linspace(0,4,41).round(2) for con_c in range(8)]).reset_index().set_index(['index'])\n",
    "jobs_fullgrid['speedup'] = [find_constantspeedup(exp_c, con_c, exp_power, con_power) for exp_power in np.linspace(0,4,41).round(2) for exp_c in range(8) for con_power in np.linspace(0,4,41).round(2) for con_c in range(8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30da1ef1-5aaf-4733-a78f-ea5a1110855e",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_per_speedup = round(((jobs_fullgrid['speedup'].value_counts().sort_index(ascending = False) * np.array([0.2, 0.4, 0.6, 0.8, 1])) / (jobs_fullgrid['speedup'].value_counts().sort_index(ascending = False) * np.array([0.2, 0.4, 0.6, 0.8, 1])).sum() * 1000) + 0.5).astype(int)\n",
    "jobs_fullgrid_split = dict()\n",
    "for speedup in jobs_per_speedup.index:\n",
    "    jobs_fullgrid_split[speedup] = pd.DataFrame(np.array_split(jobs_fullgrid.loc[jobs_fullgrid['speedup'] == speedup][0].values, jobs_per_speedup[speedup])).T.unstack()\n",
    "for speedup in [3,2,1,0]:\n",
    "    jobs_fullgrid_split[speedup].index = pd.MultiIndex.from_arrays([jobs_fullgrid_split[speedup].index.get_level_values(0) + jobs_per_speedup.cumsum()[speedup+1], jobs_fullgrid_split[speedup].index.get_level_values(1)])\n",
    "jobs_fullgrid_split = pd.concat(jobs_fullgrid_split).dropna()\n",
    "jobs_fullgrid_split.index = jobs_fullgrid_split.index.droplevel(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3bea5c-387f-4523-b768-783b468b2e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_fullgrid_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703e989f-dd9d-4eeb-8b3b-8a46f6576b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_fullgrid_split.to_pickle('simulations/fullgrid_jobs_prospeedup.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc236ec-ad41-4916-9499-2cfd12d32f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_fullgrid_split = pd.read_pickle('simulations/fullgrid_jobs_prospeedup.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d07c6d-1721-4228-b104-c94fddb15372",
   "metadata": {},
   "source": [
    "### sparse grid "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27f7c3e-d162-4e07-9256-29dba25b6bc8",
   "metadata": {},
   "source": [
    "#### for progressive speedup\n",
    "- all jobs together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5d72c4-bcbf-4189-b0c0-c330672da269",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_3Mgrid = pd.Series([(exp_c, exp_c-1, exp_power, con_power) for exp_power in np.linspace(0,4,41).round(2) for con_power in np.linspace(0,4,41).round(2) for exp_c in range(1,8)]).reset_index().set_index(['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221f8125-f7cd-4336-84a9-f15c45ea803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_3Mgrid['speedup'] = [find_constantspeedup(exp_c, exp_c-1, exp_power, con_power) for exp_power in np.linspace(0,4,41).round(2) for con_power in np.linspace(0,4,41).round(2) for exp_c in range(1,8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201f66da-2072-49ba-9f0f-706eeb98304c",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_per_speedup = round(((jobs_3Mgrid['speedup'].value_counts().sort_index(ascending = False) * np.array([0.2, 0.4, 0.6, 0.8, 1])) / (jobs_3Mgrid['speedup'].value_counts().sort_index(ascending = False) * np.array([0.2, 0.4, 0.6, 0.8, 1])).sum() * 198) + 0.5).astype(int)\n",
    "jobs_3Mgrid_split = dict()\n",
    "for speedup in jobs_per_speedup.index:\n",
    "    jobs_3Mgrid_split[speedup] = pd.DataFrame(np.array_split(jobs_3Mgrid.loc[jobs_3Mgrid['speedup'] == speedup][0].values, jobs_per_speedup[speedup])).T.unstack()\n",
    "for speedup in [3,2,1,0]:\n",
    "    jobs_3Mgrid_split[speedup].index = pd.MultiIndex.from_arrays([jobs_3Mgrid_split[speedup].index.get_level_values(0) + jobs_per_speedup.cumsum()[speedup+1], jobs_3Mgrid_split[speedup].index.get_level_values(1)])\n",
    "jobs_3Mgrid_split = pd.concat(jobs_3Mgrid_split).dropna()\n",
    "jobs_3Mgrid_split.index = jobs_3Mgrid_split.index.droplevel(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9527350-1efb-4d16-98de-84b8dfbaded2",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_3Mgrid_split.to_pickle('simulations/3Mgrid_jobs_prospeedup.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ecdf11-14cd-4ede-aa01-055201eea4ca",
   "metadata": {},
   "source": [
    "#### for constant speedup\n",
    "- separate into shorter jobs, and jobs expected to take > 12 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba2174b-42a7-473f-9377-858d992a1cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_3Mgrid = pd.Series([(exp_c, exp_c-1, exp_power, con_power) for exp_power in np.linspace(0,4,21).round(2) for con_power in np.linspace(0,4,21).round(2) for exp_c in range(1,8)]).reset_index().set_index(['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bb1c55-4dcc-47d2-ac33-8af7595ebf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_3Mgrid['speedup'] = [find_constantspeedup(exp_c, exp_c-1, exp_power, con_power) for exp_power in np.linspace(0,4,21).round(2) for con_power in np.linspace(0,4,21).round(2) for exp_c in range(1,8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ab4ea8-aa3b-4710-b473-b49d18fcbee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "slow_jobs = pd.DataFrame(np.array_split(jobs_3Mgrid.loc[jobs_3Mgrid['speedup'] == 0][0].values, 20)).unstack().dropna()\n",
    "\n",
    "fast_jobs = dict()\n",
    "fast_jobs[4] = pd.DataFrame(np.array_split(jobs_3Mgrid.loc[jobs_3Mgrid['speedup'] == 4][0].values, 2000)).unstack().dropna()\n",
    "fast_jobs[3] = pd.DataFrame(np.array_split(jobs_3Mgrid.loc[jobs_3Mgrid['speedup'] == 3][0].values, 200)).unstack().dropna()\n",
    "fast_jobs[2] = pd.DataFrame(np.array_split(jobs_3Mgrid.loc[jobs_3Mgrid['speedup'] == 2][0].values, 20)).unstack().dropna()\n",
    "fast_jobs[1] = pd.DataFrame(np.array_split(jobs_3Mgrid.loc[jobs_3Mgrid['speedup'] == 1][0].values, 20)).unstack().dropna()\n",
    "\n",
    "fast_jobs[3].index = pd.MultiIndex.from_arrays([fast_jobs[3].index.get_level_values(0) + len(fast_jobs[4].index.levels[0]), fast_jobs[3].index.get_level_values(1)])\n",
    "fast_jobs[2].index = pd.MultiIndex.from_arrays([fast_jobs[2].index.get_level_values(0) + len(fast_jobs[4].index.levels[0]) + len(fast_jobs[3].index.levels[0]), fast_jobs[2].index.get_level_values(1)])\n",
    "fast_jobs[1].index = pd.MultiIndex.from_arrays([fast_jobs[1].index.get_level_values(0) + len(fast_jobs[4].index.levels[0]) + len(fast_jobs[3].index.levels[0]) + len(fast_jobs[2].index.levels[0]), fast_jobs[1].index.get_level_values(1)])\n",
    "\n",
    "fast_jobs = pd.concat(fast_jobs)\n",
    "fast_jobs.index = fast_jobs.index.droplevel(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c12a5aa-ff06-43b6-ad85-3167b1bdf7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(slow_jobs.index.levels[0]), len(fast_jobs.index.levels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70a08e5-2ddc-4ad6-8271-565b0cb127c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "slow_jobs.to_pickle('simulations/3Mgrid_jobs_constantspeedup_medium.pickle')\n",
    "fast_jobs.to_pickle('simulations/3Mgrid_jobs_constantspeedup_short.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5bf982-5b91-43bb-af5d-ee72674e0449",
   "metadata": {},
   "source": [
    "### jobs for power law only model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888ee87d-ab19-4a0d-b29f-e890bf11bdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plonly_lambda_list = [5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "spacing = np.linspace(0,4,21).round(2)\n",
    "jobs_xaxis = pd.Series([(exp_L, con_L, exp_power, con_power) for exp_power in spacing for exp_L in plonly_lambda_list for con_power in spacing for con_L in plonly_lambda_list])\n",
    "jobs_xaxis = jobs_xaxis.sample(len(jobs_xaxis), replace=False)     # random order\n",
    "jobs_xaxis_group = dict()\n",
    "for n in range(1000):\n",
    "    jobs_xaxis_group[n] = jobs_xaxis[n::1000]\n",
    "jobs_xaxis_group = pd.concat(jobs_xaxis_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f287d6-4021-42e0-ad41-8c11e3428c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_xaxis_group.to_pickle('simulations/grid_group_jobs_xaxis_v2.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dbfdd2-fe11-4cfa-8c7a-1840a075d51e",
   "metadata": {},
   "source": [
    "### jobs for log rate curves (full grid, random order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7544e864-7356-487d-8c08-33da188ccc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_log = pd.Series([(exp_L, con_L, exp_power, con_power) for exp_power in np.linspace(0,4,41).round(2) for exp_L in range(8) for con_power in np.linspace(0,4,41).round(2) for con_L in range(8)])\n",
    "jobs_log = jobs_log.sample(len(jobs_log), replace=False)    #random order\n",
    "jobs_log_group = dict()\n",
    "for n in range(1000):\n",
    "    jobs_log_group[n] = jobs_log[n::1000]\n",
    "jobs_log_group = pd.concat(jobs_log_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b807b618-5484-4618-8a42-72daabee29c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_log_group.to_pickle('simulations/grid_group_jobs_log.pickle')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "102df37d-3a93-44a9-9dd2-e8d631a9131e",
   "metadata": {},
   "source": [
    "## Substitution-only simulations\n",
    "- note: run this set of computational models first, and process data to generate the \"subonly_counts.pickle\" file which is used to initiate runs of the computational model for all other parameter combinaztions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c44a09-498f-41c7-8624-52713be03610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory to store simulation results (create if it does not exist, can be changed as needed)\n",
    "sim_dir = 'simulations/subonly_output/'\n",
    "finished = os.listdir(sim_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2536f0d-2d87-4342-8b66-034fd598f1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutonly_jobs = [(0,0,0,0,3,10, 0.1, False, 0.01, 100, 200, 'A', True, None, None, 1000, False, 'random', True, 'simulations/subonly_output/', 'powerlaw', 0, True, None, 11, 9),\n",
    "(0,0,0,0,3,10, 0.1, False, 0.01, 100, 200, 'C', True, None, None, 1000, False, 'random', True, 'simulations/subonly_output/', 'powerlaw', 0, True, None, 11, 9),\n",
    "(0,0,0,0,3,10, 0.1, False, 0.01, 100, 500, 'AC', True, None, None, 1000, False, 'random', True, 'simulations/subonly_output/', 'powerlaw', 0, True, None, 11, 9),\n",
    "(0,0,0,0,3,10, 0.1, False, 0.01, 100, 500, 'AG', True, None, None, 1000, False, 'random', True, 'simulations/subonly_output/', 'powerlaw', 0, True, None, 11, 9),\n",
    "(0,0,0,0,3,10, 0.1, False, 0.01, 100, 500, 'AT', True, None, None, 1000, False, 'random', True, 'simulations/subonly_output/', 'powerlaw', 0, True, None, 11, 9),\n",
    "(0,0,0,0,3,10, 0.1, False, 0.01, 100, 500, 'CG', True, None, None, 1000, False, 'random', True, 'simulations/subonly_output/', 'powerlaw', 0, True, None, 11, 9),\n",
    "(0,0,0,0,3,10, 0.1, False, 0.01, 100, 500, 'AAC', True, None, None, 1000, False, 'random', True, 'simulations/subonly_output/', 'powerlaw', 0, True, None, 11, 9),\n",
    "(0,0,0,0,3,10, 0.1, False, 0.01, 100, 500, 'AAG', True, None, None, 1000, False, 'random', True, 'simulations/subonly_output/', 'powerlaw', 0, True, None, 11, 9),\n",
    "(0,0,0,0,3,10, 0.1, False, 0.01, 100, 500, 'AAT', True, None, None, 1000, False, 'random', True, 'simulations/subonly_output/', 'powerlaw', 0, True, None, 11, 9),\n",
    "(0,0,0,0,3,10, 0.1, False, 0.01, 100, 500, 'ACC', True, None, None, 1000, False, 'random', True, 'simulations/subonly_output/', 'powerlaw', 0, True, None, 11, 9),\n",
    "(0,0,0,0,3,10, 0.1, False, 0.01, 100, 500, 'ACG', True, None, None, 1000, False, 'random', True, 'simulations/subonly_output/', 'powerlaw', 0, True, None, 11, 9),\n",
    "(0,0,0,0,3,10, 0.1, False, 0.01, 100, 500, 'ACT', True, None, None, 1000, False, 'random', True, 'simulations/subonly_output/', 'powerlaw', 0, True, None, 11, 9),\n",
    "(0,0,0,0,3,10, 0.1, False, 0.01, 100, 500, 'AGC', True, None, None, 1000, False, 'random', True, 'simulations/subonly_output/', 'powerlaw', 0, True, None, 11, 9),\n",
    "(0,0,0,0,3,10, 0.1, False, 0.01, 100, 500, 'AGG', True, None, None, 1000, False, 'random', True, 'simulations/subonly_output/', 'powerlaw', 0, True, None, 11, 9),\n",
    "(0,0,0,0,3,10, 0.1, False, 0.01, 100, 500, 'ATC', True, None, None, 1000, False, 'random', True, 'simulations/subonly_output/', 'powerlaw', 0, True, None, 11, 9),\n",
    "(0,0,0,0,3,10, 0.1, False, 0.01, 100, 500, 'CCG', True, None, None, 1000, False, 'random', True, 'simulations/subonly_output/', 'powerlaw', 0, True, None, 11, 9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a3f07f-6144-4650-a6b4-23992af0bb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    pool = multiprocessing.Pool()\n",
    "    for args in mutonly_jobs:\n",
    "        pool.apply_async(run_simulation_constantspeedup, args=args)\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2564ca52-7635-4d6d-b1f7-f7ce5a1e37cf",
   "metadata": {},
   "source": [
    "#### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f95e92e-4e5d-460c-bec1-f9b2db65617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_files_mutonly = os.listdir('simulations/subonly_output/')\n",
    "grid_files_A_mutonly = [file for file in grid_files_mutonly if file.startswith('Adist')]\n",
    "grid_files_B_mutonly = [file for file in grid_files_mutonly if file.startswith('Bdist')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b3ebcc-4c1c-4980-9668-0adfbcb7afc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "substitutions_only = dict()\n",
    "for file in grid_files_A_mutonly:\n",
    "    motif = file.split('_')[1]\n",
    "    substitutions_only[motif] = pd.read_pickle('simulations/subonly_output/' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea25b9e5-5507-4408-8f40-57e110df9f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "substitutions_only_B = dict()\n",
    "for file in grid_files_B_mutonly:\n",
    "    motif = file.split('_')[1]\n",
    "    substitutions_only_B[motif] = pd.read_pickle('simulations/subonly_output/' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a20b55-003c-4a02-b20a-b6d58a113d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_subonlydist = dict()\n",
    "starting_subonlydist['A'] = dict(); starting_subonlydist['B'] = dict()\n",
    "for motif in substitutions_only:\n",
    "    starting_subonlydist['A'][motif] = substitutions_only[motif].T.iloc[-1]\n",
    "    starting_subonlydist['B'][motif] = substitutions_only_B[motif].T.iloc[-1]\n",
    "starting_subonlydist['A'] = pd.concat(starting_subonlydist['A'], axis=1)\n",
    "starting_subonlydist['B'] = pd.concat(starting_subonlydist['B'], axis=1)\n",
    "starting_subonlydist = pd.concat(starting_subonlydist, axis=1).sort_index(axis=1)\n",
    "starting_subonlydist.index +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f938274-bf87-4a36-b745-556f18cfdd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_subonlydist.to_pickle('repeat_distributions/subonly_counts_remake.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18968b64-19d2-4db5-8f04-e297c8c3d95e",
   "metadata": {},
   "source": [
    "## Run demo\n",
    "- For a demo of the computational model, the below code can be run, requiring values for \"mult\", \"exp_power\" and \"con_power\" parameters. See Methods for further description, including default values for optional paramerters.\n",
    "- Demo on a \"normal\" desktop computer with given parameters is expected to take ~10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0378dd51-1326-44e6-a2c7-d4fcb734e790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demo purposes only:\n",
    "# If data files (above) have not been created, it is possible to substitute these example data and run a demo of the computational model:\n",
    "CHM13_counts = dict()\n",
    "CHM13_counts['A'] = pd.DataFrame([861201679, 234254484, 88812020, 32801187, 12627047, 3780464, 1581788, 567751, 326510, 207206, 127161, 97987, 86634, 77081, 68340, 58903, 47652, 38435, 32238, 27356, 24095, 21886, 19548, 16572, 13746, 10757, 8409, 6853, 5435, 3908, 2740, 1824, 1408, 952, 912, 786, 809, 803, 693, 636, 497, 341, 241, 200, 163, 157, 167, 119, 116, 85, 94, 60, 42, 63, 26, 29, 20, 14, 7, 3, 3, 7, 4, 4], index = range(1,65), columns = ['A'])\n",
    "CHM13_counts['B'] = pd.DataFrame([391005710, 234634738, 173991271, 128644266, 82379768, 57163482, 44845809, 31123715, 23724537, 16138191, 12638350, 9637450, 6485630, 4960219, 3700614, 2978004, 2541592, 2199129, 1529461, 1307551, 948882, 703894, 583040, 484866, 436960, 322284, 281835, 207162, 183167, 146508, 125876, 104629, 89550, 80680, 65894, 60237, 50645, 43932, 38951, 33281, 29366, 26809, 23398, 20929, 18711, 16401, 15401, 13201, 11954, 11180, 9209, 8217, 7411, 7940, 6446, 6543, 5351, 5363, 4614, 4920, 3642, 3733, 3536, 2936, 2545, 2346, 1576, 1148, 793, 1602, 833, 2980, 2732, 2347, 2175, 1894, 1721, 1663, 1550, 1846, 1274, 1270, 1200, 1184, 863, 883, 726, 971, 667, 771, 1036, 924, 545, 982, 568, 536, 461, 578, 506, 660, 637, 453, 394, 553, 599, 371, 515, 433, 343, 501, 390, 526, 359, 333, 285, 317, 372, 327, 302, 539, 560, 518, 311, 277, 231, 297, 267, 588, 367, 361, 215, 230, 216, 402, 205, 511, 331, 215, 183, 184, 164, 340, 254, 344, 158, 401, 136, 170, 162, 187, 142, 175, 270, 235, 162, 366, 142, 351, 128, 127, 122, 111, 128, 117, 116, 136, 124, 342, 94, 112, 110, 98, 99, 134, 103, 128, 100, 149, 135, 158, 92, 83, 82, 82, 131, 191, 85, 309, 172, 396, 68, 70, 139, 278, 83, 100, 76, 149, 92, 282], index = range(1,201), columns = ['A'])\n",
    "CHM13_counts = pd.concat(CHM13_counts, axis=1)\n",
    "\n",
    "subonly_counts = dict()\n",
    "subonly_counts['A'] = pd.DataFrame([916707580.2033024, 340290716.258333, 128678308.62519115, 48838020.7874859, 18548753.855557196, 7045762.038399432, 2676403.5444367407, 1016663.2352091095, 386191.7452785688, 146699.59581817046, 55725.61361521049, 21168.04760300643, 8040.938639205142, 3054.447695047391, 1160.2688617171016, 440.7421458419924, 167.42122927793446, 63.59697677514248, 24.15808002594318, 9.176738583082912, 3.4858950269147133, 1.3241593436114223, 0.5029979255644212, 0.19106983939869215, 0.07258018705917339, 0.027570460990195812, 0.010472972721773027, 0.003978285225988993, 0.0015111997099370791, 0.0005740474685914903, 0.00021805886675958894, 8.283229519180028e-05, 3.146484813344478e-05, 1.1952302731299332e-05, 4.540226603820093e-06, 1.7246599318518845e-06, 6.551329129767819e-07, 2.4886015250820107e-07, 9.453253573385976e-08, 3.5909325869190544e-08, 1.3640591298746398e-08, 5.18154341457788e-09, 1.968271871001872e-09, 7.476718514560248e-10, 2.840121863729806e-10, 1.0788546051489958e-10, 4.098159568134373e-11, 1.55673542715905e-11, 5.9134476095456306e-12, 2.246294522548187e-12, 8.532821147996443e-13, 3.2412952091919334e-13, 1.2312451475204588e-13, 4.677033455618555e-14, 1.776627667449281e-14, 6.74873485233351e-15, 2.563588473914226e-15, 9.738100558675044e-16, 3.69913515588853e-16, 1.405161183033852e-16, 5.3376745295775186e-17, 2.0275801614578305e-17, 7.702008221664194e-18, 2.925700880991615e-18, 1.1113628288474581e-18, 4.2216459835948847e-19, 1.6036432340719404e-19, 6.091632581647316e-20, 2.3139802370857326e-20, 8.789933512660019e-21, 3.3389624474187368e-21, 1.2683452280060184e-21, 4.817962593886852e-22, 1.830161303368957e-22, 6.952088836470106e-23, 2.6408349417727886e-23, 1.0031530599987096e-23, 3.810598102390866e-24, 1.4475017299912813e-24, 5.49851021290328e-25, 2.0886755390297206e-25, 7.93408639505333e-26, 3.013858579147787e-26, 1.1448505955972263e-26, 4.348853311273552e-27, 1.6519644740485116e-27, 6.275186646154749e-28, 2.383705462510538e-28, 9.054793169731035e-29, 3.439572553323957e-29, 1.306563158508199e-29, 4.963137692955658e-30, 1.8853075015279892e-30, 7.161565258914736e-31, 2.7204041120994567e-31, 1.0333754823401774e-31, 3.9253740392858455e-32, 1.4911021301878577e-32, 5.682823327959757e-33, 3.428759707012527e-33], index = range(1,101), columns = ['A'])\n",
    "subonly_counts['B'] = pd.DataFrame([550498794.3648516, 346320080.1269246, 215422559.61719304, 133909901.19805124, 83238781.58012784, 51741505.78915495, 32162697.296232127, 19992442.949264973, 12427371.10493236, 7724896.5005180165, 4801822.158505596, 2984829.122353852, 1855380.0194100519, 1153310.58339157, 716901.8141016357, 445628.6263764576, 277004.282511156, 172186.81203998503, 107031.91290661399, 66531.40414603188, 41356.148997399345, 25707.124054394466, 15979.636478957083, 9932.99684007114, 6174.384902609636, 3838.018831515179, 2385.725668777034, 1482.9752579443784, 921.8225064420567, 573.0080315439691, 356.1837576315802, 221.4050453336747, 137.6261355238746, 85.54887785276219, 53.177475877009144, 33.05530138591516, 20.547288709994625, 12.772265132387137, 7.939283810843722, 4.935086045880151, 3.0676664117959866, 1.9068719626311865, 1.185318151897783, 0.7367978284602383, 0.45799605713836933, 0.28469191988886655, 0.17696547379996758, 0.11000234544581367, 0.06837783519997773, 0.0425038968731665, 0.02642056807035858, 0.01642311572615223, 0.01020866506111151, 0.006345741214257145, 0.003944534502529505, 0.0024519361751922765, 0.0015241319358117495, 0.0009474056385579074, 0.0005889105942086762, 0.0003660688451243766, 0.0002275496496217057, 0.00014144564262323002, 8.79230965653398e-05, 5.4653298371514975e-05, 3.3972677710072244e-05, 2.111753297938136e-05, 1.3126730925983014e-05, 8.159620963840785e-06, 5.072048375865068e-06, 3.152802665849103e-06, 1.9597929501391797e-06, 1.218214019233847e-06, 7.572460124180416e-07, 4.707067184168981e-07, 2.9259291053287115e-07, 1.8187675668200792e-07, 1.1305521572933016e-07, 7.027550983852459e-08, 4.3683497936869776e-08, 2.7153812137189442e-08, 1.6878902752873576e-08, 1.0491983840116927e-08, 6.521853138974554e-09, 4.054006278938213e-09, 2.5199842068591176e-09, 1.5664308256776053e-09, 9.736987735694173e-10, 6.052544971083951e-10, 3.762282712209035e-10, 2.338647837267046e-10, 1.453711516416722e-10, 9.036320643437254e-11, 5.617007903485778e-11, 3.4915513772451004e-11, 2.1703603109364986e-11, 1.3491034128803281e-11, 8.386073084150692e-12, 5.212811790503936e-12, 3.2403016871595538e-12, 2.014182641877817e-12, 1.2520228381568401e-12, 7.782616902133947e-13, 4.837701358111617e-13, 3.007131755882505e-13, 1.8692434129019585e-13, 1.161928116332889e-13, 7.222585021331858e-14, 4.489583620284987e-14, 2.79074057612331e-14, 1.734733913415051e-14, 1.0783165501297115e-14, 6.702852658218828e-15, 4.166516200867617e-15, 2.58992076020143e-15, 1.6099036270939265e-15, 1.000721616026816e-15, 6.220519886591289e-16, 3.8666964957855806e-16, 2.403551803242204e-16, 1.4940560442655428e-16, 9.287103612226439e-17, 5.772895456985953e-17, 3.5884516151424185e-17, 2.230593831148531e-17, 1.3865447756247434e-17, 8.61880987908215e-18, 5.357481780441848e-18, 3.3302290490741228e-18, 2.070081798464368e-18, 1.2867699456062506e-18, 7.998606113747827e-19, 4.971960992820773e-19, 3.0905880052828883e-19, 1.9211201037559625e-19, 1.1941748452872465e-19, 7.423031794465943e-20, 4.6141820219298135e-20, 2.8681913699161395e-20, 1.782877592466713e-20, 1.1082428261447667e-20, 6.88887541629852e-21, 4.282148585285124e-21, 2.661798246935351e-21, 1.6545829193632123e-21, 1.0284944173362715e-21, 6.393156572044061e-22, 3.9740080515485094e-22, 2.4702570343467985e-22, 1.5355202446965533e-22, 9.54484650418752e-23, 5.933109322599955e-23, 3.6880411034874887e-23, 2.292498998662099e-23, 1.425025240064973e-23, 8.858005765792488e-24, 5.506166763982025e-24, 3.4226521447819294e-24, 2.1275323117363145e-24, 1.322481381691934e-24, 8.2205896252379515e-25, 5.1099467048914545e-25, 3.1763603971503663e-25, 1.9744365167110993e-25, 1.2273165104374322e-25, 7.629041521686596e-26, 4.742238374913899e-26, 2.9477916381210484e-26, 1.832357392184888e-26, 1.1389996393485771e-26, 7.080060822028102e-27, 4.400990089187962e-27, 2.7356705333475412e-27, 1.700502185953998e-27, 1.057037990936665e-27, 6.570584404492206e-28, 4.0842978007157435e-28, 2.5388135206857206e-28, 1.5781351917303963e-28, 9.809742476498505e-29, 6.097769567492104e-29, 3.7903944764414215e-29, 2.3561222063277495e-29, 1.4645736441560355e-29, 9.10384000199909e-30, 5.658978168336365e-30, 3.517640237820048e-30, 2.186577236145866e-30, 1.3591839092090395e-30, 8.448733795056932e-31, 5.251761903308292e-31, 3.2645132108644712e-31, 2.0292326080501292e-31, 1.2613779487458502e-31, 7.840768590403876e-32, 4.8738486469808327e-32, 3.0296010346166e-32, 1.8832103936251097e-32, 1.1706100394524177e-32, 7.276552153203449e-33, 4.52312977454541e-33], index = range(1,201), columns = ['A'])\n",
    "subonly_counts = pd.concat(subonly_counts, axis=1)\n",
    "\n",
    "denovo_exp_rate = pd.DataFrame([4.562984199878992e-12,  1.0467617654842797e-10,  5.807374246629577e-11,  8.005268166171157e-11,  1.3679705360645337e-10,  3.0556157928111613e-10,  2.289030948565612e-09,  4.854045051592342e-09,  1.1364615491677555e-08,  1.419922964171028e-08,  1.1828716797973438e-08,  4.1291073073549976e-10,  1.447422661312361e-10,  5.0615435868574976e-11, 1.6412002445322716e-10],  index = [0.0,  1.0,  2.0,  3.0,  4.0,  5.0,  6.0,  7.0,  8.0,  9.0,  10.0,  11.0,  12.0,  13.0,  25.0], columns = ['A'])\n",
    "denovo_con_rate = pd.DataFrame([2.885531044935673e-10,  1.871202448033845e-10,  1.7746255255979686e-10,  2.498754225278858e-10,  3.7299996616692954e-10,  4.540853190555286e-10,  1.0654492863651945e-09,  3.20580338022747e-09,  7.138318574137602e-09,  1.3303065078067975e-08,  2.0285696564748838e-08,  3.1794126266633476e-09,  7.237113306561804e-10,  3.036926152114499e-10,  5.25055043000813e-11,  1.1059236545578058e-10,  1.3900672767207106e-10], [0.0,  1.0,  2.0,  3.0,  4.0,  5.0,  6.0,  7.0,  8.0,  9.0,  10.0,  11.0,  12.0,  13.0,  14.0,  20.0,  24.0], columns = ['A'])\n",
    "denovo_nonexp_rate = pd.DataFrame([1.4427655224678364e-10,  5.807374246629577e-12,  8.791913014870057e-12,  6.383862501634492e-12,  1.0406121895022531e-11,  3.3700342391113206e-11,  2.1336461765241066e-11,  1.1596546420079138e-10,  1.3940337656658055e-10,  2.763718878031177e-10,  2.890375115148498e-10,  3.377319543062175e-10,  3.036926152114499e-10,  2.625275215004065e-10,  1.6588854818367085e-10,  6.036395505019818e-11,  6.996035254753307e-11,  8.281119434384756e-11,  9.221806429413932e-11, 1.1676038583818848e-10, 1.6412002445322716e-10, 1.0262244516086404e-09], index = [0.0,  2.0,  3.0,  4.0,  5.0,  6.0,  7.0,  8.0,  9.0,  10.0,  11.0,  12.0,  13.0,  14.0,  15.0,  16.0,  17.0,  18.0,  19.0, 22.0,  25.0, 32.0], columns = ['A'])\n",
    "denovo_substitution_context_rate = pd.DataFrame([4.541573240083719e-09,  8.077760235763368e-09,  6.447333857760922e-09,  2.865765641003364e-09,  4.7677112600042175e-09,  3.967453912936841e-09], index = ['Afission', 'Acontraction', 'A10', 'Afusion', 'Aexpansion', 'A01'], columns = ['A']).transpose()\n",
    "\n",
    "intercept_list = dict()\n",
    "intercept_list['A'] = [denovo_con_rate['A'][8]] + [denovo_exp_rate['A'][8] * (denovo_exp_rate['A'][8]/ denovo_con_rate['A'][8])**x for x in range(7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a6effe-8976-4dd2-ab1c-393260d79ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory to store simulation results (create if it does not exist, can be changed as needed)\n",
    "sim_dir = 'simulations/testing/'\n",
    "finished = os.listdir(sim_dir)\n",
    "test = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64688902-07d6-482c-b220-59ffba6acb1c",
   "metadata": {},
   "source": [
    "#### with constant speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f83c54a-a6dc-445e-9b65-12dd812c470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[0] = run_simulation_constantspeedup(3,2,0.5,1, ceiling = 0.1, min_speedup = 4, input_nuc = 'A', mutonly = False, rounds = 9, interp = True, A_bins = 200, B_bins = 200, starting_counts = 'subonly', overwrite=True, sim_dir=sim_dir, first_bin=1, recnum = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcdfc69-7258-4fe5-97d6-aaf90d26c598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo of multiprocessing\n",
    "grid_group = pd.Series([(0, 0, 0.0, 0.0), (0, 1, 0.0, 0.0), (0, 2, 0.0, 0.0), (0, 3, 0.0, 0.0)])\n",
    "if __name__ == '__main__':\n",
    "    pool = multiprocessing.Pool()\n",
    "    for exp_i, con_i, exp_p, con_p in grid_group:\n",
    "        pool.apply_async(run_simulation_constantspeedup, args=(exp_i, con_i, exp_p, con_p, 2, 5, 0.1))\n",
    "    pool.close()\n",
    "    pool.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dd336e-8ef9-4a95-903a-22c77ca20eac",
   "metadata": {},
   "source": [
    "#### with progressive speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb93dad1-5917-43bc-86d6-0d83fd41996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[1] = run_simulation_prospeedup(3,2,0.5,1, min_speedup=4, starting_counts = 'subonly', sim_dir=sim_dir, overwrite = True, interp = False, first_bin = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5359e9-4a6a-42a1-b9a3-9eef6acf1ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo of multiprocessing\n",
    "grid_group = pd.Series([(0, 0, 0.0, 0.0), (0, 1, 0.0, 0.0), (0, 2, 0.0, 0.0), (0, 3, 0.0, 0.0)])\n",
    "if __name__ == '__main__':\n",
    "    pool = multiprocessing.Pool(processes=8)\n",
    "    for exp_i, con_i, exp_p, con_p in grid_group:\n",
    "        pool.apply_async(run_simulation_prospeedup, args=(exp_i, con_i, exp_p, con_p, 5, True))\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98220fd-c059-4e71-b824-91e71cd1e3ba",
   "metadata": {},
   "source": [
    "#### Demo plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8e4d36-7e40-4933-aaba-97ed7d8ccdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_counts = pd.read_pickle('repeat_distributions/bootstrap_counts_1000.pickle').sort_index()\n",
    "bootstrap_counts_max = dict(); bootstrap_counts_min = dict(); bootstrap_counts_mean = dict()\n",
    "for motif in bootstrap_counts.columns.levels[2]:\n",
    "    bootstrap_counts_mean[motif] = bootstrap_counts.xs(len(motif), level=1, drop_level=True, axis=1).xs(motif, level=1, drop_level=True, axis=1).apply(lambda x: x.sort_values().head(975).tail(950).mean(), axis=1).sort_index().replace(0, np.nan).dropna()\n",
    "    bootstrap_counts_max[motif] = bootstrap_counts.xs(len(motif), level=1, drop_level=True, axis=1).xs(motif, level=1, drop_level=True, axis=1).apply(lambda x: x.sort_values().head(975).tail(950).max(), axis=1).reindex(bootstrap_counts_mean[motif].index)\n",
    "    bootstrap_counts_min[motif] = bootstrap_counts.xs(len(motif), level=1, drop_level=True, axis=1).xs(motif, level=1, drop_level=True, axis=1).apply(lambda x: x.sort_values().head(975).tail(950).min(), axis=1).reindex(bootstrap_counts_mean[motif].index)\n",
    "\n",
    "import plotly     # https://plotly.com/python/getting-started/\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"none\"\n",
    "pd.set_option('display.precision', 3)\n",
    "\n",
    "def make_colorscale(list_of_traces, opacity = 0.15):\n",
    "    current_colors = pd.Series(['rgb('+str(current/len(list_of_traces)*255) + ', 180, '+str((len(list_of_traces)-current)/len(list_of_traces)*255)+')' for current in range(len(list_of_traces))], index = list_of_traces)\n",
    "    return pd.Series(current_colors, index = list_of_traces), pd.Series(['rgba' + color[3:-1] + ', '+str(opacity)+')' for color in current_colors], index = list_of_traces)\n",
    "def loop_list(input_list, count):\n",
    "    return input_list * (count // len(input_list)) + input_list[:(count % len(input_list))]\n",
    "    \n",
    "def make_evolve_fig(input_df, motif = 'A'):\n",
    "    fig = go.Figure()\n",
    "    colors = make_colorscale([rep for rep in input_df.columns] , opacity= 0.3)\n",
    "    reps = input_df.columns[1:]\n",
    "    \n",
    "    fig.add_trace(go.Scattergl(x = input_df.index + 1, y = input_df[0] / input_df[0].sum(), name = 'start', legendgroup = 'start', connectgaps = True, line = dict(color = 'gray', dash = 'dot')))\n",
    "    for rep in reps:\n",
    "        fig.add_trace(go.Scattergl(x = input_df.index + 1, y = input_df[rep] / input_df[rep].sum(), name = '{:0.1e}'.format(rep), mode = 'lines', line = dict(color = colors[1][rep] if rep != reps[-1] else colors[0][rep])))\n",
    "    fig.add_trace(go.Scatter(x = bootstrap_counts_mean[motif].index, y = bootstrap_counts_mean[motif] / bootstrap_counts_mean[motif].sum(), line = dict(color = 'rgba(0,0,0,0.8)'), legendgroup = 'ci', name = 'T2T-CHM13'))\n",
    "    fig.add_trace(go.Scatter(x = bootstrap_counts_max[motif].index, y = bootstrap_counts_max[motif] / bootstrap_counts_mean[motif].sum(), line = dict(color = 'rgba(0,0,0,0)'), legendgroup = 'ci', showlegend = False, name = 'CHM13 bootstrap 95%'))\n",
    "    fig.add_trace(go.Scatter(x = bootstrap_counts_min[motif].index, y = bootstrap_counts_min[motif] / bootstrap_counts_mean[motif].sum(), fill='tonexty', fillcolor = 'rgba(0,0,0,0.25)', line = dict(color = 'rgba(0,0,0,0)'), legendgroup = 'ci', showlegend = False, name = 'CHM13 bootstrap 5%'))\n",
    "\n",
    "    fig.update_xaxes(type = 'log', title = 'repeat length (units)', range = [0,2])\n",
    "    fig.update_yaxes(type = 'log', title = 'repeat counts (normalized)', range = [-9.05,0.05], tickformat = '0.1e', dtick = 2)\n",
    "        \n",
    "    fig.update_layout(margin={'t':20,'l':60,'b':40,'r':10})        \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab47eba-0e70-48c7-84ea-6f23c3bfc5d5",
   "metadata": {},
   "source": [
    "##### compare constant and progressive speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d250360c-d345-4865-a1b9-703918d392f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_evolve_fig(test[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7281ca5d-4398-4291-91c0-3cb4a3a9793a",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_evolve_fig(test[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2308ecb6-ff65-4d9a-a6b6-bf1c22706c9c",
   "metadata": {},
   "source": [
    "## Plot comparison of initial states (Fig. S13b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b363212e-a4bd-4f97-9c7e-afc70fcc735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly     # https://plotly.com/python/getting-started/\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1304574-64a1-4a77-9fc0-ab638cf5bef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fig. S13b (left)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x = subonly_counts['A']['A'].index, y = subonly_counts['A']['A'], line = dict(width = 3, color = 'black'), name = 'geometric<br>initial state'))\n",
    "fig.add_trace(go.Scatter(x = counts_uniform['A']['A'].index, y = counts_uniform['A']['A'], line = dict(dash = 'dash', width = 4, color = 'red'), name = 'compound<br>initial state'))\n",
    "fig.update_xaxes(type = 'log', range = [-0.1,1.9], title = 'repeat tract length (units)', tickvals = [1,5,10,20,70])\n",
    "fig.update_yaxes(type = 'log', title = 'counts', range = [0,9.1], tickformat = '1.0e', )\n",
    "fig.update_layout(width = 400, height = 400, legend = dict(xref = 'paper', x = 0.54))\n",
    "fig.update_layout(font=dict(family = 'Arial', size = 16), margin={'t':50,'l':70,'b':45,'r':10})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac549d9-2aed-4ed5-bf23-0db77bfae82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.write_image('plots/initial_state.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
